# -*- coding: utf-8 -*-
"""deeplearning_final.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15zAVdxzEl7Dz-IZRi-KcewSJ-xTlbHId
"""

from google.colab import drive
drive.mount('/content/drive')

import os
import zipfile

# COCO 데이터셋 경로
zip_path = '/content/drive/MyDrive/Colab Notebooks/COCO.zip'
extract_path = './COCO'

# 압축 해제
if not os.path.exists(extract_path):
    with zipfile.ZipFile(zip_path, 'r') as zip_ref:
        zip_ref.extractall(extract_path)
else:
    print("Already extracted")

"""### Dataset 전처리"""

import os
import glob
from PIL import Image
import torch
from torch.utils.data import Dataset
from torchvision.transforms.functional import to_tensor

class CocoFullImageDataset(Dataset):
    def __init__(self, img_dir, k=4, target_size=512):
        self.img_paths = sorted(glob.glob(os.path.join(img_dir, '*.png')))
        self.k = k
        self.target_size = target_size

    def __len__(self):
        return len(self.img_paths)

    def __getitem__(self, idx):
        # Load & Resize
        hr_img = Image.open(self.img_paths[idx]).convert('RGB')
        hr_img = hr_img.resize((self.target_size, self.target_size), Image.BICUBIC)

        # Down → Up sampling
        low_res_size = self.target_size // self.k
        lr_img = hr_img.resize((low_res_size, low_res_size), Image.BICUBIC)
        lr_up_img = lr_img.resize((self.target_size, self.target_size), Image.BICUBIC)

        # To tensor
        input_tensor = to_tensor(lr_up_img)   # Low resolution (upsampled)
        target_tensor = to_tensor(hr_img)     # Ground truth

        return input_tensor, target_tensor

# dataset
k = 4 # 해상도 축소 배율
trainset = CocoFullImageDataset(img_dir='./COCO/Train', k=k, target_size=512)

print(f"# of trainset = {len(trainset)}")

input, target = trainset[0]
print(f"input size: {input.shape}, target size: {target.shape}")  # → [3, 512, 512] expected

# Dataloader
batchsize = 16 #속도 향상을 위해 수정
trainset_small = torch.utils.data.Subset(trainset, range(4500))  # 작은 trainset을 사용하여 빠르게 학습 결과 확인 가능
trainloader = torch.utils.data.DataLoader(trainset_small,       #trainset -> trainset_small
                                          batch_size=batchsize,
                                          shuffle=True,    #매 epoch 마다 데이터는 랜덤하게
                                          num_workers=2,
                                          drop_last=True) # 마지막 미니배치가 batch_size보다 작을 경우, 해당 배치를 버림

"""### RegionSelector 정의"""

import torch.nn as nn
import torch
import torch.nn.functional as F
from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights

# 128x128 이미지를 입력받아 16x16 크기의 중요도 맵(heatmap)을 출력하는 RegionSelector
class RegionSelector(nn.Module):
    def __init__(self):
        super(RegionSelector, self).__init__()
        self.model = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(2),  # maxpool 방식으로 중요한 특징이 강한 부분을 선정
            nn.Conv2d(32, 64, kernel_size=3, padding=1),
            nn.ReLU(),
            nn.MaxPool2d(4),
            nn.Conv2d(64, 1, kernel_size=1),
            nn.Sigmoid()
        )

    def forward(self, x):
        return self.model(x)

# heatmap: [B, 1, H, W] → binary mask: [B, 1, H, W]
def generate_binary_mask(heatmap, k):
    B, C, H, W = heatmap.shape  # btch size, channel, height, width
    mask = torch.zeros_like(heatmap)

    #print(f"generate_binary_mask: heatmap shape = {heatmap.shape}, top-k = {k}")

    for i in range(B):
        flat = heatmap[i].view(-1)
        topk = torch.topk(flat, k).indices
        mask[i].view(-1)[topk] = 1.0

    return mask

# 이미지와 마스크를 받아 마스크된 이미지 반환
def apply_mask_to_image(image, mask, patch_size):

    B, C, H, W = image.shape
    _, _, Hm, Wm = mask.shape

    # 1. unfold (이미지를 패치 단위로 분해)
    patches = torch.nn.functional.unfold(image, kernel_size=patch_size, stride=patch_size)  # [B, C*ks*ks, L]

    # 2. flatten mask (binary mask를 unfold된 위치에 맞춤)
    mask_flat = mask.view(B, -1).unsqueeze(1)  # [B, 1, L]

    # 3. 마스킹 적용
    masked_patches = patches * mask_flat  # [B, C*ks*ks, L]

    # 4. fold로 다시 이미지 형태로 복원
    masked_image = torch.nn.functional.fold(
        masked_patches, output_size=(H, W), kernel_size=patch_size, stride=patch_size
    )

    #print(f" 완료: masked image shape = {masked_image.shape}")
    return masked_image  # 중요한 패치만 유지되고 나머지는 제거된 이미지 형태

# ViT 기반 RegionSelector2
class ViTRegionSelector(nn.Module):
    def __init__(self, patch_size=16, num_classes=256):
        super(ViTRegionSelector, self).__init__()

        weights = ViT_B_16_Weights.DEFAULT
        self.vit = vit_b_16(weights=weights)

        in_features = self.vit.heads[0].in_features
        self.vit.heads = nn.Linear(in_features, num_classes)
        self.activation = nn.Sigmoid()

    def forward(self, x): # x:[B,3,224,224]
        #print(f" ViTRegionSelector: 입력 크기 = {x.shape}")
        logits = self.vit(x)
        heatmap = self.activation(logits).view(-1, 1, 16, 16)  #[B,1,16,16]
        return F.interpolate(heatmap, scale_factor=2, mode='bilinear') #[B,1,32,32]

"""### 모델 정의"""

# 모델 정의
import torch
import torch.nn as nn

class MultiInputSRCNN(nn.Module):
    def __init__(self):
        super(MultiInputSRCNN, self).__init__()

        # 입력 분기 정의
        self.base_branch = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=5, padding=2),
            nn.ReLU(inplace=True),
        )

        self.ref256_branch = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

        self.ref512_branch = nn.Sequential(
            nn.Conv2d(3, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
        )

        # 통합 및 복원
        self.fusion = nn.Sequential(
            nn.Conv2d(96, 64, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(64, 32, kernel_size=3, padding=1),
            nn.ReLU(inplace=True),
            nn.Conv2d(32, 3, kernel_size=3, padding=1)
        )

    def forward(self, x128_up, x256_masked_up, x512_masked, mask_256_up, mask_512):

        # 특징 추출
        base_feat = self.base_branch(x128_up)  # [B, 32, 512, 512]
        ref256_feat = self.ref256_branch(x256_masked_up)  # [B, 32, 512, 512]
        ref512_feat = self.ref512_branch(x512_masked)     # [B, 32, 512, 512]
        mask_512_up = F.interpolate(mask_512, size=(512, 512), mode='nearest')  # or mode='bilinear'


        # 마스크로 픽셀 중요도 조절
        ref256_feat = ref256_feat * mask_256_up           # [B, 32, 512, 512]
        ref512_feat = ref512_feat * mask_512_up           # [B, 32, 512, 512]

        # 통합
        fused = torch.cat([base_feat, ref256_feat, ref512_feat], dim=1)  # [B, 96, 512, 512]
        out = self.fusion(fused)
        return out.clamp(0, 1)
device = 'cuda' if torch.cuda.is_available() else 'cpu'
#print("device : ", device)

model = MultiInputSRCNN().to(device)

"""# 학습"""

import torch.optim as optim
from tqdm import tqdm
import numpy as np
import torch.nn.functional as F
from torchvision.models.vision_transformer import vit_b_16, ViT_B_16_Weights

def train(model, device, train_loader, optimizer, criterion, num_epochs,
          region_selector1, region_selector2, k1, k2, scheduler=None):

    history = np.zeros((0,2))  # (epoch, loss) 저장

    model.train()
    region_selector1.eval()  # RegionSelector는 학습시키지 않음
    region_selector2.eval()

    for epoch in range(num_epochs):
        epoch_loss = 0

        for inputs, targets in train_loader:
            inputs, targets = inputs.to(device), targets.to(device)

            # 1단계: 128 → 16x16 마스크 → 256 이미지에 적용
            input_128 = F.interpolate(inputs, size=(128, 128), mode='bicubic')
            input_256 = F.interpolate(input_128, size=(256, 256), mode='bicubic')
            with torch.no_grad():
                heatmap_256 = region_selector1(input_128) # 16x16 크기의 중요도 맵(heatmap)을 출력 => 256 % patch_size(16) = 16 따라서 16*16 개의 패치에 대응
                mask_256 = generate_binary_mask(heatmap_256, k1) # 25% 정보량을 담기 위해서는 k1 = 64 -> 256 / 64 = 25%
                input_256_masked = apply_mask_to_image(input_256, mask_256, patch_size=16)

            # 2단계: 256 → 32x32 마스크 → 512 이미지에 적용
            input_224 = F.interpolate(input_128, size=(224, 224), mode='bicubic')
            with torch.no_grad():
                heatmap_512 = region_selector2(input_224)  # 출력:[B, 1, 32, 32] => 512 % patch_size(16) = 32 따라서 32*32 개의 패치에 대응
                mask_512 = generate_binary_mask(heatmap_512, k2) # 6.25% 정보량을 담기 위해서는 K2 = 64 -> 1024 /64 = 6.25%
                input_512_masked = apply_mask_to_image(inputs, mask_512, patch_size=16)

            # 128 이미지는 업샘플해서 512로 맞추기
            input_128_up = F.interpolate(input_128, size=(512, 512), mode='bicubic')

            # 모델 forward
            mask_256_up = F.interpolate(mask_256, size=(512, 512), mode='nearest')
            input_256_masked_up = F.interpolate(input_256_masked, size=(512, 512), mode='bicubic')
            outputs = model(input_128_up, input_256_masked_up, input_512_masked, mask_256_up, mask_512)
            loss = criterion(outputs, targets)

            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()

        avg_loss = epoch_loss / len(train_loader)
        history = np.vstack((history, np.array([epoch+1, avg_loss])))

        if scheduler:
            scheduler.step()

        print(f"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_loss:.6f}")

    return history

num_epochs = 20
k1 = 64
k2 = 64
model = MultiInputSRCNN().to(device)
region_selector1 = RegionSelector().to(device)
region_selector2 = ViTRegionSelector().to(device)

# optimizer
learning_rate = 1e-3
optimizer = optim.Adam(model.parameters(), lr=learning_rate)
#optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)

# shceduler(learning rate 조절)
scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5) # 40 epoch마다 0.1배 -> 30 dp 0.5로 일단 수정
#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)

# loss fuction(MSE)
criterion = nn.MSELoss() # 최종 프로젝트서는 psnr의 역수가 mseloss이기 때문에 mseloss를 낮추는 목적으로 진행


print("학습 시작")
print(f"전체 trainset 크기: {len(trainset_small)}개 샘플")
history = train(model, device, trainloader,
                optimizer, criterion, num_epochs,
                region_selector1, region_selector2, k1, k2,
                scheduler=scheduler)

import matplotlib.pyplot as plt
print(f'initial state : loss : {history[0,1]:.5f}')
print(f'final state : loss : {history[-1,1]:.5f}')

num_epochs = len(history)
unit = num_epochs / 10

plt.figure(figsize=(6,5))
plt.plot(history[:,0], history[:,1], 'b', label='train')
plt.xticks(np.arange(0,num_epochs+1, unit))
plt.xlabel('epoch')
plt.ylabel('loss')
plt.title('learning curve(loss)')
plt.legend()
plt.show()

"""## test

"""

import os
import random
from PIL import Image
import matplotlib.pyplot as plt
import torch
import torch.nn.functional as F
from torchvision.transforms.functional import to_tensor, to_pil_image

# PSNR 계산 함수
def compute_psnr(x1, x2):
    mse = torch.mean((x1 - x2) ** 2)
    psnr = 10 * torch.log10(1.0 / (mse + 1e-8))
    return psnr

# 테스트 이미지 목록 준비
test_dir = './COCO/Test'
img_files = sorted([f for f in os.listdir(test_dir) if f.endswith('.jpg') or f.endswith('.png')])
assert len(img_files) >= 1, "COCO/Test 폴더에 이미지가 없습니다."
print(f"총 테스트 이미지 수: {len(img_files)}")

# 대표 이미지 1장 선정
representative_img = random.choice(img_files)
print(f"대표 이미지: {os.path.join(test_dir, representative_img)}")

# 누적 변수 초기화
psnr_total_bicubic = 0.0
psnr_total_srcnn = 0.0
rep_result = {}

# 모델 추론 모드
model.eval()
region_selector1.eval()
region_selector2.eval()

for idx, fname in enumerate(img_files):
    hr = Image.open(os.path.join(test_dir, fname)).convert('RGB').resize((512, 512), Image.BICUBIC)
    lr_128 = hr.resize((128, 128), Image.BICUBIC)

    # 텐서 변환
    lr_128_tensor = to_tensor(lr_128).unsqueeze(0).to(device)

    # 128 기반으로 모든 해상도 생성
    lr_256_tensor = F.interpolate(lr_128_tensor, size=(256, 256), mode='bicubic')
    lr_224_tensor = F.interpolate(lr_128_tensor, size=(224, 224), mode='bicubic')
    lr_up_tensor = F.interpolate(lr_128_tensor, size=(512, 512), mode='bicubic')

    with torch.no_grad():
        # 1단계 마스크: 16×16 출력
        heatmap_256 = region_selector1(lr_128_tensor)  # [B, 1, 16, 16]
        mask_256 = generate_binary_mask(heatmap_256, 64)
        input_256_masked = apply_mask_to_image(lr_256_tensor, mask_256, patch_size=16)

        # 2단계 마스크: 32×32 출력
        heatmap_512 = region_selector2(lr_224_tensor)  # [B, 1, 32, 32]
        mask_512 = generate_binary_mask(heatmap_512, 64)
        input_512_masked = apply_mask_to_image(lr_up_tensor, mask_512, patch_size=16)

        # 마스크 업샘플링
        mask_256_up = F.interpolate(mask_256, size=(512, 512), mode='nearest')
        mask_512_up = F.interpolate(mask_512, size=(512, 512), mode='nearest')

        # 모델 입력 준비
        input_128_up = F.interpolate(lr_128_tensor, size=(512, 512), mode='bicubic')
        input_256_masked_up = F.interpolate(input_256_masked, size=(512, 512), mode='bicubic')
        output = model(input_128_up, input_256_masked_up, input_512_masked, mask_256_up, mask_512_up).squeeze(0).cpu().clamp(0, 1)

        # PSNR 계산
        psnr_total_bicubic += compute_psnr(to_tensor(hr), lr_up_tensor.squeeze(0).cpu().clamp(0, 1)).item()
        psnr_total_srcnn += compute_psnr(to_tensor(hr), output).item()

        # 대표 이미지 저장
        if fname == representative_img:
            rep_result = {
                'hr': hr,
                'lr_up': to_pil_image(lr_up_tensor.squeeze(0).cpu().clamp(0, 1)),
                'output': output,
                'masked_256': to_pil_image(input_256_masked.squeeze(0).cpu().clamp(0, 1)),
                'masked_512': to_pil_image(input_512_masked.squeeze(0).cpu().clamp(0, 1)),
                'psnr_bicubic': compute_psnr(to_tensor(hr), lr_up_tensor.squeeze(0).cpu().clamp(0, 1)),
                'psnr_srcnn': compute_psnr(to_tensor(hr), output)
            }

    if (idx + 1) % 100 == 0:
        print(f"진행 중... {idx + 1}/{len(img_files)}")

# 평균 PSNR 출력
print(f"\n평균 PSNR - Bicubic: {psnr_total_bicubic / len(img_files):.3f} dB")
print(f"평균 PSNR - SRCNN:   {psnr_total_srcnn / len(img_files):.3f} dB")

# 대표 이미지 시각화
plt.figure(figsize=(15, 6))

plt.subplot(2, 3, 1)
plt.imshow(rep_result['hr'])
plt.title("Original", fontsize=14)
plt.axis("off")

plt.subplot(2, 3, 2)
plt.imshow(rep_result['lr_up'])
plt.title(f"Bicubic / {rep_result['psnr_bicubic']:.2f} dB", fontsize=14)
plt.axis("off")

plt.subplot(2, 3, 3)
plt.imshow(to_pil_image(rep_result['output']))
plt.title(f"SRCNN / {rep_result['psnr_srcnn']:.2f} dB", fontsize=14)
plt.axis("off")

plt.subplot(2, 3, 4)
plt.imshow(rep_result['masked_256'])
plt.title("Masked 256×256", fontsize=14)
plt.axis("off")

plt.subplot(2, 3, 5)
plt.imshow(rep_result['masked_512'])
plt.title("Masked 512×512", fontsize=14)
plt.axis("off")

plt.tight_layout()
plt.show()